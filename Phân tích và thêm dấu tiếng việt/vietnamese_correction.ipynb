{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTfZT-LcJVke",
        "outputId": "561415bb-449f-4b4b-a337-4ab811c11dc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "class FileData(object):\n",
        "    def __init__(self, path):\n",
        "        self.path = path\n",
        "        with open(path, encoding='utf-16') as f:\n",
        "          self.data = f.read()\n",
        "          #print(self.data)\n",
        "\n",
        "ABSOLUTE_PATH = r\"/content/drive/MyDrive/vnese-spell-correction-lstm/Train_Full\"\n",
        "\n",
        "c_tri =  \"/Chinh tri Xa hoi\"\n",
        "\n",
        "doi_song = \"/Doi song\"\n",
        "\n",
        "khoa_hoc = \"/Khoa hoc\"\n",
        "\n",
        "kinh_doanh = \"/Kinh doanh\"\n",
        "\n",
        "p_luat = \"/Phap luat\"\n",
        "\n",
        "suc_khoe = \"/Suc khoe\"\n",
        "\n",
        "the_gioi = \"/The gioi\"\n",
        "\n",
        "the_thao = \"/The thao\"\n",
        "\n",
        "van_hoa = \"/Van hoa\"\n",
        "\n",
        "vi_tinh = \"/Vi tinh\"\n",
        "corpus = [c_tri, doi_song, khoa_hoc, kinh_doanh, p_luat, suc_khoe, the_gioi, the_thao, van_hoa, vi_tinh]\n",
        "\n",
        "#Extract folder path\n",
        "for folder_path in range(len(corpus)):\n",
        "    corpus[folder_path] = ABSOLUTE_PATH + corpus[folder_path]\n",
        "\n",
        "file_list = []\n",
        "\n",
        "#Extracting text from corpus\n",
        "for folder_path in corpus:\n",
        "    count = 0\n",
        "    for name in os.listdir(folder_path):\n",
        "        count +=1\n",
        "        if count == 1500:\n",
        "          break\n",
        "        path = os.path.join(folder_path, name)\n",
        "        if not os.path.isfile(path):\n",
        "            continue\n",
        "        file = FileData(path)\n",
        "        file_list.append( file.data )\n",
        "\n",
        "print('Corpus length: ', len(file_list))\n",
        "\n",
        "#Save extracted corpus as Pickle file\n",
        "path_corpus = r\"/content/drive/MyDrive/vnese-spell-correction-lstm/train_corpus.pkl\"\n",
        "\n",
        "with open(path_corpus, 'wb') as pickle_file:\n",
        "    pickle.dump(file_list, pickle_file)"
      ],
      "metadata": {
        "id": "uE5ODaF6JKpe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unidecode"
      ],
      "metadata": {
        "id": "jU_Wh_hwJOHB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2281127-2069-423f-ce55-d3b74753e111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/235.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/235.5 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/235.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import pickle\n",
        "from unidecode import unidecode\n",
        "import itertools\n",
        "from nltk import ngrams\n",
        "from tqdm import tqdm\n",
        "\n",
        "path_corpus = r\"/content/drive/MyDrive/vnese-spell-correction-lstm/train_corpus.pkl\"\n",
        "\n",
        "with open(path_corpus, \"rb\") as f:\n",
        "    data = pickle.load(f)\n",
        "\n",
        "alphabet = '^[ _abcdefghijklmnopqrstuvwxyz0123456789áàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđ!\\\"\\',\\-\\.:;?_\\(\\)]+$'\n",
        "\n",
        "#Extracting sentence from corpus\n",
        "def latin_extract(data):\n",
        "\n",
        "    # extract Latin- characters only\n",
        "    latin_extract_data=[]\n",
        "    # duyet qua tung van ban\n",
        "    for i in data:\n",
        "      if i == 1:\n",
        "        break\n",
        "      # thay the xuong dong la dau cham ket thuc\n",
        "      i=i.replace(\"\\n\",\".\")\n",
        "      # tach van ban theo dau cham ket thuc\n",
        "      sentences=i.split(\".\")\n",
        "      for j in sentences:\n",
        "          if len(j.split()) > 2 and re.match(alphabet, j.lower()):\n",
        "\n",
        "              latin_extract_data.append(j)\n",
        "\n",
        "    return latin_extract_data\n",
        "\n",
        "training_data = latin_extract(data)\n",
        "i = 100\n",
        "#Listing all typos, regional dialects\n",
        "letters=list(\"abcdefghijklmnopqrstuvwxyzáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđABCDEFGHIJKLMNOPQRSTUVWXYZÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÉÈẺẼẸÊẾỀỂỄỆÚÙỦŨỤƯỨỪỬỮỰÍÌỈĨỊÝỲỶỸỴĐ\")\n",
        "letters2=list(\"abcdefghijklmnopqrstuvwxyz\")\n",
        "\n",
        "typo={\"ă\":\"aw\",\"â\":\"aa\",\"á\":\"as\",\"à\":\"af\",\"ả\":\"ar\",\"ã\":\"ax\",\"ạ\":\"aj\",\"ắ\":\"aws\",\"ổ\":\"oor\",\"ỗ\":\"oox\",\"ộ\":\"ooj\",\"ơ\":\"ow\",\n",
        "\"ằ\":\"awf\",\"ẳ\":\"awr\",\"ẵ\":\"awx\",\"ặ\":\"awj\",\"ó\":\"os\",\"ò\":\"of\",\"ỏ\":\"or\",\"õ\":\"ox\",\"ọ\":\"oj\",\"ô\":\"oo\",\"ố\":\"oos\",\"ồ\":\"oof\",\n",
        "\"ớ\":\"ows\",\"ờ\":\"owf\",\"ở\":\"owr\",\"ỡ\":\"owx\",\"ợ\":\"owj\",\"é\":\"es\",\"è\":\"ef\",\"ẻ\":\"er\",\"ẽ\":\"ex\",\"ẹ\":\"ej\",\"ê\":\"ee\",\"ế\":\"ees\",\"ề\":\"eef\",\n",
        "\"ể\":\"eer\",\"ễ\":\"eex\",\"ệ\":\"eej\",\"ú\":\"us\",\"ù\":\"uf\",\"ủ\":\"ur\",\"ũ\":\"ux\",\"ụ\":\"uj\",\"ư\":\"uw\",\"ứ\":\"uws\",\"ừ\":\"uwf\",\"ử\":\"uwr\",\"ữ\":\"uwx\",\n",
        "\"ự\":\"uwj\",\"í\":\"is\",\"ì\":\"if\",\"ỉ\":\"ir\",\"ị\":\"ij\",\"ĩ\":\"ix\",\"ý\":\"ys\",\"ỳ\":\"yf\",\"ỷ\":\"yr\",\"ỵ\":\"yj\",\"đ\":\"dd\",\n",
        "\"Ă\":\"Aw\",\"Â\":\"Aa\",\"Á\":\"As\",\"À\":\"Af\",\"Ả\":\"Ar\",\"Ã\":\"Ax\",\"Ạ\":\"Aj\",\"Ắ\":\"Aws\",\"Ổ\":\"Oor\",\"Ỗ\":\"Oox\",\"Ộ\":\"Ooj\",\"Ơ\":\"Ow\",\n",
        "\"Ằ\":\"AWF\",\"Ẳ\":\"Awr\",\"Ẵ\":\"Awx\",\"Ặ\":\"Awj\",\"Ó\":\"Os\",\"Ò\":\"Of\",\"Ỏ\":\"Or\",\"Õ\":\"Ox\",\"Ọ\":\"Oj\",\"Ô\":\"Oo\",\"Ố\":\"Oos\",\"Ồ\":\"Oof\",\n",
        "\"Ớ\":\"Ows\",\"Ờ\":\"Owf\",\"Ở\":\"Owr\",\"Ỡ\":\"Owx\",\"Ợ\":\"Owj\",\"É\":\"Es\",\"È\":\"Ef\",\"Ẻ\":\"Er\",\"Ẽ\":\"Ex\",\"Ẹ\":\"Ej\",\"Ê\":\"Ee\",\"Ế\":\"Ees\",\"Ề\":\"Eef\",\n",
        "\"Ể\":\"Eer\",\"Ễ\":\"Eex\",\"Ệ\":\"Eej\",\"Ú\":\"Us\",\"Ù\":\"Uf\",\"Ủ\":\"Ur\",\"Ũ\":\"Ux\",\"Ụ\":\"Uj\",\"Ư\":\"Uw\",\"Ứ\":\"Uws\",\"Ừ\":\"Uwf\",\"Ử\":\"Uwr\",\"Ữ\":\"Uwx\",\n",
        "\"Ự\":\"Uwj\",\"Í\":\"Is\",\"Ì\":\"If\",\"Ỉ\":\"Ir\",\"Ị\":\"Ij\",\"Ĩ\":\"Ix\",\"Ý\":\"Ys\",\"Ỳ\":\"Yf\",\"Ỷ\":\"Yr\",\"Ỵ\":\"Yj\",\"Đ\":\"Dd\"}\n",
        "\n",
        "# dia phuong\n",
        "region={\"ẻ\":\"ẽ\",\"ẽ\":\"ẻ\",\"ũ\":\"ủ\",\"ủ\":\"ũ\",\"ã\":\"ả\",\"ả\":\"ã\",\"ỏ\":\"õ\",\"õ\":\"ỏ\",\"i\":\"j\"}\n",
        "region2={\"s\":\"x\",\"l\":\"n\",\"n\":\"l\",\"x\":\"s\",\"d\":\"gi\",\"S\":\"X\",\"L\":\"N\",\"N\":\"L\",\"X\":\"S\",\"Gi\":\"D\",\"D\":\"Gi\"}\n",
        "\n",
        "# nguyen am\n",
        "vowel=list(\"aeiouyáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵ\")\n",
        "\n",
        "# viet tat\n",
        "acronym={\"không\":\"ko\",\" anh\":\" a\",\"em\":\"e\",\"biết\":\"bít\",\"giờ\":\"h\",\"gì\":\"j\",\"muốn\":\"mún\",\"học\":\"hok\",\"yêu\":\"iu\",\n",
        "         \"chồng\":\"ck\",\"vợ\":\"vk\",\" ông\":\" ô\",\"được\":\"đc\",\"tôi\":\"t\",\n",
        "         \"Không\":\"Ko\",\" Anh\":\" A\",\"Em\":\"E\",\"Biết\":\"Bít\",\"Giờ\":\"H\",\"Gì\":\"J\",\"Muốn\":\"Mún\",\"Học\":\"Hok\",\"Yêu\":\"Iu\",\n",
        "         \"Chồng\":\"Ck\",\"Vợ\":\"Vk\",\" Ông\":\" Ô\",\"Được\":\"Đc\",\"Tôi\":\"T\",}\n",
        "\n",
        "# teencode\n",
        "teen={\"ch\":\"ck\",\"ph\":\"f\",\"th\":\"tk\",\"nh\":\"nk\",\n",
        "      \"Ch\":\"Ck\",\"Ph\":\"F\",\"Th\":\"Tk\",\"Nh\":\"Nk\"}\n",
        "\n",
        "# function for adding mistake( noise)\n",
        "def teen_code(sentence, pivot):\n",
        "    random = np.random.uniform(0,1,1)[0]\n",
        "    new_sentence=str(sentence)\n",
        "    if random>pivot:\n",
        "        for word in acronym.keys():\n",
        "            if re.search(word, new_sentence):\n",
        "                random2 = np.random.uniform(0,1,1)[0]\n",
        "                if random2 <0.5:\n",
        "                    new_sentence=new_sentence.replace(word,acronym[word])\n",
        "        for word in teen.keys():\n",
        "            if re.search(word, new_sentence):\n",
        "                random3 = np.random.uniform(0,1,1)[0]\n",
        "                if random3 <0.05:\n",
        "                    new_sentence=new_sentence.replace(word,teen[word])\n",
        "        return new_sentence\n",
        "    else:\n",
        "        return sentence\n",
        "\n",
        "\n",
        "def add_noise(sentence, pivot1,pivot2):\n",
        "    sentence=teen_code(sentence,0.5)\n",
        "    noisy_sentence = \"\"\n",
        "    i = 0\n",
        "    while i < len(sentence):\n",
        "        if sentence[i] not in letters:\n",
        "            noisy_sentence+=sentence[i]\n",
        "        else:\n",
        "            random = np.random.uniform(0,1,1)[0]\n",
        "            if random < pivot1:\n",
        "                noisy_sentence+=(sentence[i])\n",
        "            elif random<pivot2:\n",
        "                if sentence[i] in typo.keys() and sentence[i] in region.keys():\n",
        "                    random2=np.random.uniform(0,1,1)[0]\n",
        "                    if random2<=0.4:\n",
        "                        noisy_sentence+=typo[sentence[i]]\n",
        "                    elif random2<0.8:\n",
        "                        noisy_sentence+=region[sentence[i]]\n",
        "                    elif random2<0.95 :\n",
        "                        noisy_sentence+=unidecode(sentence[i])\n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "                elif sentence[i] in typo.keys():\n",
        "                    random3=np.random.uniform(0,1,1)[0]\n",
        "                    if random3<=0.6:\n",
        "                        noisy_sentence+=typo[sentence[i]]\n",
        "                    elif random3<0.9 :\n",
        "                        noisy_sentence+=unidecode(sentence[i])\n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "                elif sentence[i] in region.keys():\n",
        "                    random4=np.random.uniform(0,1,1)[0]\n",
        "                    if random4<=0.6:\n",
        "                        noisy_sentence+=region[sentence[i]]\n",
        "                    elif random4<0.85 :\n",
        "                        noisy_sentence+=unidecode(sentence[i])\n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "                elif i<len(sentence)-1 :\n",
        "                    if sentence[i] in region2.keys() and (i==0 or sentence[i-1] not in letters) and sentence[i+1] in vowel:\n",
        "                        random5=np.random.uniform(0,1,1)[0]\n",
        "                        if random5<=0.9:\n",
        "                            noisy_sentence+=region2[sentence[i]]\n",
        "                        else:\n",
        "                            noisy_sentence+=sentence[i]\n",
        "                    else:\n",
        "                        noisy_sentence+=sentence[i]\n",
        "\n",
        "            else:\n",
        "                new_random = np.random.uniform(0,1,1)[0]\n",
        "                if new_random <=0.33:\n",
        "                    if i == (len(sentence) - 1):\n",
        "                        continue\n",
        "                    else:\n",
        "                        noisy_sentence+=(sentence[i+1])\n",
        "                        noisy_sentence+=(sentence[i])\n",
        "                        i += 1\n",
        "                elif new_random <= 0.66:\n",
        "                    random_letter = np.random.choice(letters2, 1)[0]\n",
        "                    noisy_sentence+=random_letter\n",
        "                else:\n",
        "                    pass\n",
        "\n",
        "        i += 1\n",
        "    return noisy_sentence\n",
        "\n",
        "def extract_phrases(text):\n",
        "    return re.findall(r'\\w[\\w ]+', text)\n",
        "\n",
        "def _extract_phrases(data):\n",
        "    phrases = itertools.chain.from_iterable(extract_phrases(text) for text in data)\n",
        "    phrases = [p.strip() for p in phrases if len(p.split()) > 1]\n",
        "\n",
        "    return phrases\n",
        "\n",
        "phrases = _extract_phrases(training_data)\n",
        "\n",
        "#Generate Bi-gram\n",
        "\n",
        "#A Vietnamese word do not contain more than 7 characters, so an bi-gram do not have more than 15 characters\n",
        "NGRAM = 2\n",
        "MAXLEN = 40\n",
        "\n",
        "def gen_ngrams(words, n=2):\n",
        "    return ngrams(words.split(), n)\n",
        "\n",
        "def generate_bi_grams(phrases):\n",
        "    list_ngrams = []\n",
        "    for p in tqdm(phrases):\n",
        "\n",
        "      # neu khong nham trong bang chu cai thi bo qua\n",
        "      if not re.match(alphabet, p.lower()):\n",
        "        continue\n",
        "\n",
        "      # tach p thanh cac bi gram\n",
        "      for ngr in gen_ngrams(p, NGRAM):\n",
        "        if len(\" \".join(ngr)) < MAXLEN:\n",
        "          list_ngrams.append(\" \".join(ngr))\n",
        "\n",
        "    return list_ngrams\n",
        "\n",
        "list_ngrams = generate_bi_grams(phrases)\n",
        "\n",
        "print(len(list_ngrams))\n",
        "\n",
        "alphabet = ['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n",
        "\n",
        "def encoder_data(text, maxlen=MAXLEN):\n",
        "        #print(\"Maxlen\", maxlen)\n",
        "        text = \"\\x00\" + text\n",
        "        #print(\"text\", text)\n",
        "        x = np.zeros((maxlen, len(alphabet)))\n",
        "        #print(\"X ban dau\", x)\n",
        "        for i, c in enumerate(text[:maxlen]):\n",
        "            x[i, alphabet.index(c)] = 1\n",
        "        if i < maxlen - 1:\n",
        "          for j in range(i+1, maxlen):\n",
        "            x[j, 0] = 1\n",
        "        return x\n",
        "\n",
        "def decoder_data(x):\n",
        "    x = x.argmax(axis=-1)\n",
        "    #print(\"x hien tai\", x)\n",
        "    dem = ''.join(alphabet[i] for i in x)\n",
        "    #print(\"Do dai cau van\", len(dem))\n",
        "\n",
        "    return dem\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcmD4AXHJpPj",
        "outputId": "483572b9-a5d7-469d-811e-c1788b096d3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 666584/666584 [00:17<00:00, 37161.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5793398\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[0:4])\n",
        "print(training_data[0:4])\n",
        "print(phrases[0:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZCslCBXg7k7",
        "outputId": "b3240682-d55b-4b25-a247-9662fa41cfa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Các nhà khoa học nói về ô nhiễm tại hố chôn gà dịch \\nTrong những ngày qua, có nhiều ý kiến phản ánh của người dân về tình trạng ô nhiễm tại các điểm chôn gà chết của TP HCM. Ở các hố chôn, nước dịch trào ra, thoát khí nặng mùi, nhất là vào những hôm trời nóng bức. VnExpress đã trao đổi với một số nhà khoa học và quản lý môi trường về vấn đề này.\\nGiáo sư, Tiến sĩ Tưởng Thị Hội, giảng viên Viện Khoa học và công nghệ Môi trường, ĐH Bách Khoa Hà Nội: Động vật khi chôn sâu trong lòng đất sẽ bị phân hủy dưới dạng hiếu khí và yếm khí, tạo ra những hợp chất dạng khác nhau như thể lỏng hoặc thể khí. Các dịch lỏng phát sinh có thể ngấm vào đất và gây ô nhiễm nguồn nước. Điều này trước kia cũng đã xảy ra ở nghĩa trang Văn Điển, Hà Nội. Các khí tạo thành trong quá trình phân hủy chất hữu cơ như CH4, NH3, H2S, CO2 thoát ra ngoài môi trường sẽ tạo ra mùi hôi, gây độc hại đến môi trường sống.\\nGiáo sư Nguyễn Công Mẫn, Phó chủ tịch Viện Địa kỹ thuật thuộc Liên hiệp các Hội khoa học kỹ thuật Việt Nam: Các chất hữu cơ khi phân hủy trong các bãi chôn lấp sẽ tạo ra nước rác trong đó có chứa các hợp chất hữu cơ, canxi, xương& Đặc biệt, đối với các hố chôn gà chết do dịch thì còn chứa cả các vi trùng gây bệnh. Do vậy, cần phải có biện pháp thích hợp để xử lý. Thông thường, các hố chôn lấp phải có đường thoát nước rác riêng dẫn ra nơi xử lý, nếu không chúng sẽ ngấm vào trong đất, phát tán ra môi trường xung quanh. Khả năng nguy cơ gây ô nhiễm nước ngầm tuỳ thuộc vào tính chất địa tầng ở từng nơi. Điều này phải kiểm tra và xét nghiệm trên thực tế mới có kết quả chính xác.\\nBà Đoàn Thị Tới, Trưởng phòng Quản lý môi trường, Sở Tài nguyên  Môi trường TP HCM: Chúng tôi cùng với Trung tâm Y tế dự phòng đã tiến hành lấy mẫu tại một số điểm chôn gà để có kết quả chính xác về nguy cơ ô nhiễm nước. Theo đúng kỹ thuật thì sau khi lấy mẫu 5 ngày mới bắt đầu xét nghiệm và 7 ngày sau mới có kết quả. Nói chung, các hố chôn đều tại vị trí cao, khô ráo, chôn theo đúng quy trình kỹ thuật nên khả năng ô nhiễm là không lớn. Chỉ có tại khu vực huyện Nhà Bè có là vùng thấp, ngập nước nên cần phải có thời gian để có kết luận chính xác về sự ô nhiễm nước mặt. Còn các giếng khoan tại đây có độ sâu khoảng 100 m trong khi hố chôn chỉ có 1-2 m nên nguy cơ ô nhiễm nước ngầm rất khó xảy ra. Về môi trường đất, do thời gian chôn chưa lâu nên chưa phát hiện được dấu hiệu ô nhiễm. Những trường hợp người dân phản ánh về tình trạng mùi xú uế phát sinh từ các hố chôn, đây là điều bình thường trong quá trình phân huỷ chất hữu cơ. Chỉ trong một thời gian ngắn là sẽ không còn hiện tượng này nữa. Hiện tại, chúng tôi cùng với Trung tâm Y tế dự phòng chỉ tiến hành đắp thêm đất, rải thêm vôi để giảm ô nhiễm.\\nGiáo sư Hoàng Thuỷ Long trả lời trực tuyến về dịch cúm A(30/01/2004)\\nMột chủ trại gà ở Long An chết vì bệnh cúm(30/01/2004)\\nCác trường bán trú 'tẩy chay' thịt gia cầm(29/01/2004)\\nChim cảnh vẫn được mua bán tại TP HCM(29/01/2004)\\nPhát hiện thêm 15 ổ dịch cúm gà mới(29/01/2004)\\n\\n\", 'Thêm 12 doanh nghiệp được đưa lao động sang Malaysia\\nNgày 28/1, Bộ Lao động Thương binh và Xã hội ra quyết định cho phép 12 doanh nghiệp đưa người lao động sang làm việc tại Malaysia. Như vậy, sau 2 năm hợp tác với nước bạn, đến nay có 82 doanh nghiệp Việt Nam được đưa nhân công sang Malaysia.\\nDanh sách các doanh nghiệp như sau: \\n1. Tổng công ty Xây dựng Hà Nội - HACC2. Tổng công ty Du lịch Sài Gòn - SAIGON TOURIST3. Công ty Kinh doanh sản xuất Sài Gòn - Đăk Lăk - SADACO4. Công ty Cổ phần Thương mại Châu Hưng - CHAUHUNG JSC5. Công ty Du lịch Hà Nội - HANOI TOURISM6. Công ty Xuất nhập khẩu Tổng hợp II - GENERALIMEX7. Công ty Thương mại Tổng hợp Bà Rịa - Vũng Tàu - GETRACO8. Công ty Xây dựng, Dịch vụ và Hợp tác lao động - OLECO9. Tổng Công ty Xây dựng miền Trung - COSEVCO10. Công ty TNHH Quốc Dân - QUOCDAN Company Limited11. Tổng Công ty Lắp máy Việt Nam - LILAMA12. Công ty Hợp tác đào tạo và Xuất khẩu lao động - LETCO\\nBên cạnh việc cho phép nhiều đơn vị tham gia thị trường lao động Malaysia, Cục Quản lý lao động ngoài nước, Bộ Lao động Thương binh và Xã hội, yêu cầu: doanh nghiệp kiểm tra kỹ điều kiện của hợp đồng, bảo đảm phù hợp với những quy định của pháp luật Việt Nam và pháp luật Malaysia. Hồ sơ hợp đồng phải được thẩm định tại Ban Quản lý lao động Việt Nam tại Malaysia.\\nNgoài ra, các đơn vị phải đăng ký hợp đồng với Cục Quản lý lao động ngoài nước theo quy định hiện hành; phải phối hợp với các cơ quan và chính quyền địa phương để tổ chức tuyển chọn trực tiếp, đúng đối tượng; tổ chức đào tạo giáo dục định hướng cho người lao động, đặc biệt lưu ý về phong tục, tập quán truyền thống văn hóa và các quy định về ngoại ngữ, giao tiếp của phía Malaysia.\\n\\n', 'Hàng hạ giá hút khách đi lễ phủ Tây Hồ\\nCon đường vào phủ Tây Hồ (Hà Nội) chỉ 200 m nhưng có đến vài chục gian hàng quây tạm, ngồn ngộn những đống quần áo, mũ, túi xách, giầy dép... đại hạ giá. Nhiều người đến phủ bị hấp dẫn bởi không khí bán mua nhốn nháo mà quên mất mục đích chính là đi lễ.\\nNgười bán, kẻ mua say sưa lựa chọn, mặc cả. Theo những người bán hàng, đây toàn là \"hàng nhà máy\", bị lỗi đôi chút nên không xuất khẩu được, đem ra đây bán giá chỉ còn 50.000-80.000 đồng/bộ quần áo thể thao, các loại mũ giá chỉ có 4.000 đồng/chiếc, khăn, găng tay, túi xách cũng rẻ tương tự. Người mua cũng thấy thoải mái bởi đây là giá hời trong khi Tết ra cái gì cũng đang đắt đỏ, giá cao gấp đôi gấp ba ngày thường.\\n\"Mấy ngày nay, hàng của tôi rất đắt khách. Đầu năm đi lễ lại mua được hàng rẻ thì ai mà chẳng thích!\", một chị bán hàng cười hả hê, nói. Bên những đống quần áo, nhiều người kẹp cả vàng hương vào nách để lục tìm, lựa chọn. Một khách hàng sau khi trả tiền cho mấy thứ đồ vừa mua, ngẩng lên kêu giời vì đã sẩm tối mà vẫn chưa vào phủ lễ được.\\nBác Trịnh Văn Thương, ở phố Trần Khát Trân, Hà Nội, đang đứng chờ vợ và cô con gái mua đồ, ngán ngẩm nói với VnExpress: \"Tôi thấy để cái chợ tự phát này ở đây không hợp lý. Sự nhốn nháo của chợ sẽ làm xấu đi hình ảnh của phủ Tây Hồ trong mắt du khách, đặc biệt là khách nước ngoài\". Phủ Tây Hồ là nơi thờ Bà chúa Liễu - một trong 4 vị thần \"Tứ bất tử\" của Việt Nam.\\nTheo ông Hà Văn Dũng, nhân viên Ban quản lý phủ Tây Hồ, từ ngày 30 Tết đến nay đã xảy ra hơn 30 trường hợp người đi lễ bị móc túi. Bọn lưu manh thường lợi dụng sự nhốn nháo, lộn xộn ở những gian hàng bán quần áo để hành nghề. Tuy nhiên, Ban Quản lý chỉ có trách nhiệm đảm bảo an ninh trật tự từ cổng phủ trở vào. Việc người dân được phép buôn bán quanh lối vào phủ cũng như đảm bảo trật tự ngoài cổng, trách nhiệm lại thuộc về UBND phường Quảng An. \\n\\n', 'Hải Phòng cấm xuất, nhập gia cầm trên địa bàn\\nTừ hôm nay, tất cả các tổ chức, cá nhân không được phép vận chuyển gia cầm vào Hải Phòng và từ thành phố này ra những tỉnh khác. Trước đó, quy định \"nội bất xuất, ngoại bất nhập\" mới chỉ được áp dụng ở 18 tỉnh phía Nam, theo công điện của Bộ Nông nghiệp và Phát triển nông thôn.\\nPhó chủ tịch UBND Hải Phòng Vũ Chí Thanh vừa yêu cầu các ban, ngành phải vào cuộc tích cực hơn nữa nhằm ngăn chặn nguy cơ lây lan dịch cúm gà tại thành phố Cảng, đặc biệt là vấn đề kiểm soát việc vận chuyển gia cầm vào thành phố. Hai trạm kiếm soát lớn được đặt tại Cầu Nghìn (trên quốc lộ 10) và Trạm thu phí Quán Toan (trên quốc lộ 5). Tại 2 trạm này, lực lượng Thú y thành phố sẽ phối hợp cùng ngành công an, quản lý thị trường ngăn chặn mọi hoạt động vận chuyển gia cầm ra, vào thành phố.\\nChi cục phó Thú y Hải Phòng Phạm Thị Thuận cho biết, đối với các trường hợp vận chuyển bằng đường nhỏ sẽ giao cho các huyện, xã kiểm soát. Lực lượng thú y sẽ phối hợp với công an địa phương xử lý các trường hợp vận chuyển gia cầm. Các đơn vị, hộ nông dân bị thiệt hại do dịch bệnh sẽ được hỗ trợ 5.000-10.000 đồng/con. \\nTrong khi Hải Phòng đã cấm xuất, nhập gia cầm thì một số tỉnh phía Bắc như Hà Nội, Hà Tây, Hải Dương, Bắc Ninh, Thanh Hóa mới chỉ cấm vận chuyển gà bệnh và cho lưu thông gia cầm khỏe mạnh. Theo ông Nguyễn Đình Tiến, quyền đội trưởng Kiểm dịch lưu động Hà Tây, các chốt đều gặp khó khăn khi thực hiện biện pháp này, vì lực lượng ngoài thú y không hỗ trợ được nhiều. \"Nếu là lệnh cấm nội bất xuất, ngoại bất nhập thì công an, quân đội... cũng có thể làm được. Còn để nhận biết gà có bệnh hay không thì ngay cả cán bộ thú y cũng còn gặp khó khăn\", ông Tiến giải thích. \\nÔng Tiến lấy ví dụ về chốt kiểm dịch ở Ba La (thị xã Hà Đông), được lập từ ngày 21/1 (30 Tết Nguyên đán). 2 cán bộ thú y ở đây không thể phân ca trực, vì khi có nhiều xe cùng vận chuyển gia cầm qua chốt, 1 một người không thể tiến hành kiểm dịch đầy đủ được. Ngoài ra, nếu chủ hàng ngụy trang kín đáo, hoặc nếu người trực quá mệt mỏi thì cũng khó phát hiện được. \"Nhiều khi họ đi qua rồi, cán bộ thú y mới phát hiện xe có chở gà, thì cũng chỉ biết đứng nhìn. Chúng tôi không có điện thoại để báo chốt sau chặn xe chở gia cầm đó lại\", ông Tiến nói.\\nTheo thông tin của VnExpress, hầu hết chốt kiểm dịch của các tỉnh có dịch phía Bắc đều trong tình trạng trên. Ngay ở trạm kiểm dịch động vật Ngọc Hồi, một trong hai trạm lớn nhất của Hà Nội, có tới 4 cán bộ thú y, nhưng lưu lượng vận chuyển gia cầm lớn nên công việc kiểm dịch cũng làm không xuể, dù chỉ là phun hóa chất khử trùng rồi cho qua, hoặc bắt quay trở lại. \"Chúng tôi kiểm dịch bằng mắt thường, nên chỉ có thể phát hiện gà bệnh khi triệu chứng đã rõ ràng. Do vậy, rất khó đảm bảo ngăn chặn được 100% gia cầm bị cúm, nhất là khi chúng đang trong thời gian ủ bệnh\", một cán bộ kiểm dịch tại trạm Ngọc Hồi cho biết.\\nTrong khi đó, dịch cúm gà đang diễn biến hết sức phức tạp ở các tỉnh phía Bắc, đặc biệt là thời tiết giá lạnh. Một số chuyên gia thú y cho rằng, thời gian ủ bệnh của virus có thể lên tới 20-30 ngày, do đó, gà mang virus nhưng chưa có triệu chứng vẫn có thể được vận chuyển tự do trên thị trường nhiều tỉnh. \\n\\n']\n",
            "['Các nhà khoa học nói về ô nhiễm tại hố chôn gà dịch ', 'Trong những ngày qua, có nhiều ý kiến phản ánh của người dân về tình trạng ô nhiễm tại các điểm chôn gà chết của TP HCM', ' Ở các hố chôn, nước dịch trào ra, thoát khí nặng mùi, nhất là vào những hôm trời nóng bức', ' VnExpress đã trao đổi với một số nhà khoa học và quản lý môi trường về vấn đề này']\n",
            "['Các nhà khoa học nói về ô nhiễm tại hố chôn gà dịch', 'Trong những ngày qua', 'có nhiều ý kiến phản ánh của người dân về tình trạng ô nhiễm tại các điểm chôn gà chết của TP HCM', 'Ở các hố chôn', 'nước dịch trào ra']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "def build_vocab(sentences):\n",
        "    tokenizer = Tokenizer(filters='')\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    vocab = tokenizer.word_index\n",
        "    return vocab\n",
        "\n",
        "# Sử dụng hàm để xây dựng vocab từ danh sách các câu văn\n",
        "vocab = build_vocab(phrases)\n",
        "\n",
        "# In vocab\n",
        "print(\"Vocab:\")\n",
        "print(len(vocab))\n",
        "\n"
      ],
      "metadata": {
        "id": "sjpqokS0g95x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5881617-84c8-4209-b494-1a8b263f5ffc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab:\n",
            "39029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nomb-I6Mg92i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kKdvT1Ejg9y9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow tensorflow-addons"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mmybWft-bIo",
        "outputId": "de09668b-ba2b-47eb-d390-1303ba913af1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: typeguard, tensorflow-addons\n",
            "Successfully installed tensorflow-addons-0.23.0 typeguard-2.13.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class TransformerEncoderModel(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, max_seq_length, dropout=0.1):\n",
        "        super(TransformerEncoderModel, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, dropout, max_seq_length)\n",
        "\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(d_model, nhead),\n",
        "            num_layers=num_encoder_layers\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.positional_encoding(x)\n",
        "\n",
        "        output = self.transformer_encoder(x)\n",
        "        output = output.mean(dim=0)  # Global average pooling\n",
        "\n",
        "        return self.fc(output)\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=512):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "\n",
        "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(0), :]\n",
        "        return self.dropout(x)\n",
        "\n",
        "# Tham số của mô hình\n",
        "vocab_size = len(alphabet)  # Kích thước từ vựng\n",
        "d_model = 256  # Kích thước embedding\n",
        "nhead = 8  # Số lượng attention heads trong mỗi layer\n",
        "num_encoder_layers = 6  # Số lượng layer trong encoder\n",
        "max_seq_length = MAXLEN  # Độ dài tối đa của chuỗi đầu vào\n",
        "\n",
        "# Tạo và compile mô hình\n",
        "transformer_model = TransformerEncoderModel(\n",
        "    vocab_size=vocab_size,\n",
        "    d_model=d_model,\n",
        "    nhead=nhead,\n",
        "    num_encoder_layers=num_encoder_layers,\n",
        "    max_seq_length=max_seq_length\n",
        ")\n",
        "\n",
        "# In thông tin về mô hình\n",
        "print(transformer_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xW9WTrABA7oZ",
        "outputId": "079bde3c-d465-4d7c-a94a-db242b085e6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TransformerEncoderModel(\n",
            "  (embedding): Embedding(199, 256)\n",
            "  (positional_encoding): PositionalEncoding(\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (transformer_encoder): TransformerEncoder(\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiheadAttention(\n",
            "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "        )\n",
            "        (linear1): Linear(in_features=256, out_features=2048, bias=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (linear2): Linear(in_features=2048, out_features=256, bias=True)\n",
            "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "        (dropout1): Dropout(p=0.1, inplace=False)\n",
            "        (dropout2): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (fc): Linear(in_features=256, out_features=199, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "  def __init__(self, embed_dim, numh_heads, ff_dim, rate=0.1):\n",
        "    super().__init__\n",
        "    self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "    self.ffn = keras.Sequential(\n",
        "        [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "    )\n",
        "    self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.dropout1 = layers.Dropout(rate)\n",
        "    self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    attn_output = self.att(inputs, inputs)\n",
        "    attn_output = self.dropout1(attn_output, training = training)\n",
        "    out1 = self.layernorm1(inputs + attn_output)\n",
        "    ffn_output = self.ffn(out1)\n",
        "    ffn_output = self.dropout2(ffn_output, training = training)\n",
        "    return self.layernorm2(out1, ffn_output)\n"
      ],
      "metadata": {
        "id": "urXPRc5pPBr5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "  def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "    super().__init__\n",
        "    self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "    self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "  def call(self, x):\n",
        "    maxlen = tf.shape(x)[-1]\n",
        "    positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "    positions = self.pos_emb(positions)\n",
        "    x = self.token_emb(x)\n",
        "    return x + positions\n"
      ],
      "metadata": {
        "id": "vgdx_frzQ2bb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "sequence_len = 200\n",
        "embed_dim = 128\n",
        "num_heads = 6\n",
        "ff_dim = 128\n",
        "\n",
        "inputs = layers.Input(shape=(sequence_len,))\n",
        "embedding_layer = TokenAndPositionEmbedding(sequence_len, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dims)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "JkJ_-aUAcIQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "sequence_len = 200\n",
        "embed_dim = 128\n",
        "num_heads = 6\n",
        "ff_dim = 128\n",
        "vocab_size = 10000  # Replace with the actual vocabulary size\n",
        "\n",
        "inputs = layers.Input(shape=(sequence_len,))\n",
        "embedding_layer = TokenAndPositionEmbedding(sequence_len, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
        "\n",
        "model = models.Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lr=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n",
        "\n",
        "# Split training data\n",
        "train_data, valid_data = train_test_split(list_ngrams, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define batch size\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "# Define data generation method\n",
        "def generate_data(data, batch_size):\n",
        "    cur_index = 0\n",
        "    while True:\n",
        "        x, y = [], []\n",
        "        for i in range(batch_size):\n",
        "            y.append(encoder_data(data[cur_index]))\n",
        "            x.append(encoder_data(add_noise(data[cur_index], 0.94, 0.985)))\n",
        "            cur_index += 1\n",
        "            if cur_index > len(data) - 1:\n",
        "                cur_index = 0\n",
        "        yield np.array(x), np.array(y)\n",
        "\n",
        "# Create data generators for training and validation\n",
        "train_generator = generate_data(train_data, batch_size=BATCH_SIZE)\n",
        "validation_generator = generate_data(valid_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Train the model and save the best weights\n",
        "checkpointer = ModelCheckpoint(filepath=os.path.join('/content/drive/MyDrive/vnese-spell-correction-lstm/spelling_ver1.h5'),\n",
        "                               save_best_only=True, verbose=1)\n",
        "\n",
        "# Fit the model\n",
        "model.fit(train_generator, steps_per_epoch=len(train_data)//BATCH_SIZE, epochs=5,\n",
        "          validation_data=validation_generator, validation_steps=len(valid_data)//BATCH_SIZE,\n",
        "          callbacks=[checkpointer])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "86m0Itk9E2Sv",
        "outputId": "4b735658-cd62-4809-9418-989b75cb4194"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 200)]             0         \n",
            "                                                                 \n",
            " token_and_position_embeddi  (None, 200, 128)          1305600   \n",
            " ng (TokenAndPositionEmbedd                                      \n",
            " ing)                                                            \n",
            "                                                                 \n",
            " transformer_block (Transfo  (None, 200, 128)          429184    \n",
            " rmerBlock)                                                      \n",
            "                                                                 \n",
            " global_average_pooling1d (  (None, 128)               0         \n",
            " GlobalAveragePooling1D)                                         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                4128      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1738978 (6.63 MB)\n",
            "Trainable params: 1738978 (6.63 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation, TimeDistributed, Dense,LSTM, Bidirectional\n",
        "from keras.callbacks import Callback, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Sửa lỗi trong layer SpellCorrectionTransformer\n",
        "class SpellCorrectionTransformer(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(SpellCorrectionTransformer, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.ff_dim = ff_dim\n",
        "        self.rate = rate\n",
        "\n",
        "        # Sửa đổi key_dim để phản ánh kích thước của đầu vào\n",
        "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim // num_heads)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            layers.Dense(embed_dim)\n",
        "        ])\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "# Assume you have vocab_size and sequence_len defined\n",
        "vocab_size = 10000\n",
        "sequence_len = 200\n",
        "\n",
        "# Create the model\n",
        "embed_dim = 128\n",
        "num_heads = 6\n",
        "ff_dim = 128\n",
        "\n",
        "inputs = layers.Input(shape=(sequence_len,))\n",
        "embedding_layer = TokenAndPositionEmbedding(sequence_len, vocab_size, embed_dim)\n",
        "x = embedding_layer(inputs)\n",
        "transformer_block = SpellCorrectionTransformer(embed_dim, num_heads, ff_dim)\n",
        "x = transformer_block(x)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "x = layers.Dense(32, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.1)(x)\n",
        "outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
        "model.summary()\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(learning_rate=0.001),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Split training data\n",
        "train_data, valid_data = train_test_split(list_ngrams, test_size=0.2, random_state=42)\n",
        "\n",
        "# Define batch size\n",
        "BATCH_SIZE = 512\n",
        "\n",
        "# Define data generation method\n",
        "def generate_data(data, batch_size):\n",
        "    cur_index = 0\n",
        "    while True:\n",
        "        x, y = [], []\n",
        "        for i in range(batch_size):\n",
        "            y.append(encoder_data(data[cur_index]))\n",
        "            x.append(encoder_data(add_noise(data[cur_index], 0.94, 0.985)))\n",
        "            cur_index += 1\n",
        "            if cur_index > len(data) - 1:\n",
        "                cur_index = 0\n",
        "        yield np.array(x), np.array(y)\n",
        "\n",
        "# Create data generators for training and validation\n",
        "train_generator = generate_data(train_data, batch_size=BATCH_SIZE)\n",
        "validation_generator = generate_data(valid_data, batch_size=BATCH_SIZE)\n",
        "\n",
        "# Train the model and save the best weights\n",
        "checkpointer = ModelCheckpoint(filepath=os.path.join('/content/drive/MyDrive/vnese-spell-correction-lstm/spelling_transformer.h5'),\n",
        "                               save_best_only=True, verbose=1)\n",
        "\n",
        "\n",
        "model.fit(train_data, steps_per_epoch=len(train_data)//BATCH_SIZE, epochs=5,\n",
        "          validation_data=valid_data, validation_steps=len(valid_data)//BATCH_SIZE,\n",
        "          callbacks=[checkpointer])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4scbfkKGWhL",
        "outputId": "170a7de4-9997-4190-ae85-2d44281db5ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 200)]             0         \n",
            "                                                                 \n",
            " token_and_position_embeddi  (None, 200, 128)          1305600   \n",
            " ng_3 (TokenAndPositionEmbe                                      \n",
            " dding)                                                          \n",
            "                                                                 \n",
            " spell_correction_transform  (None, 200, 128)          98554     \n",
            " er_3 (SpellCorrectionTrans                                      \n",
            " former)                                                         \n",
            "                                                                 \n",
            " global_average_pooling1d_3  (None, 128)               0         \n",
            "  (GlobalAveragePooling1D)                                       \n",
            "                                                                 \n",
            " dropout_14 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 32)                4128      \n",
            "                                                                 \n",
            " dropout_15 (Dropout)        (None, 32)                0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 10000)             330000    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1738282 (6.63 MB)\n",
            "Trainable params: 1738282 (6.63 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMRjkZdpEm2y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56f43541-b06d-43f1-f356-a3921ecd3f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N gram [('tôi', 'teen'), ('teen', 'laf'), ('laf', 'Tô'), ('Tô', 'Văn'), ('Văn', 'Tus'), ('Tus', 'tooi'), ('tooi', 'ddang'), ('ddang', 'hojc'), ('hojc', 'lớp'), ('lớp', '5')]\n",
            "guess ['tôi tên', 'tên là', 'là Tô', 'Tô Văn', 'Văn Tú', 'Tú ttôi', 'tôi đang', 'đang học', 'học lớp', 'lớp 5']\n",
            "tôi tên là Tô Văn Tú ttôi đang học lớp 5\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "from keras.models import load_model\n",
        "from nltk import ngrams,word_tokenize\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "\n",
        "model = load_model('/content/drive/MyDrive/vnese-spell-correction-lstm/spelling_ver1.h5')\n",
        "\n",
        "NGRAM=2\n",
        "MAXLEN=40\n",
        "alphabet = ['\\x00', ' ', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ', 'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'í', 'ì', 'ỉ', 'ĩ', 'ị', 'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'đ', 'Á', 'À', 'Ả', 'Ã', 'Ạ', 'Â', 'Ấ', 'Ầ', 'Ẩ', 'Ẫ', 'Ậ', 'Ă', 'Ắ', 'Ằ', 'Ẳ', 'Ẵ', 'Ặ', 'Ó', 'Ò', 'Ỏ', 'Õ', 'Ọ', 'Ô', 'Ố', 'Ồ', 'Ổ', 'Ỗ', 'Ộ', 'Ơ', 'Ớ', 'Ờ', 'Ở', 'Ỡ', 'Ợ', 'É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ê', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ư', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị', 'Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'Đ']\n",
        "letters=list(\"abcdefghijklmnopqrstuvwxyzáàảãạâấầẩẫậăắằẳẵặóòỏõọôốồổỗộơớờởỡợéèẻẽẹêếềểễệúùủũụưứừửữựíìỉĩịýỳỷỹỵđABCDEFGHIJKLMNOPQRSTUVWXYZÁÀẢÃẠÂẤẦẨẪẬĂẮẰẲẴẶÓÒỎÕỌÔỐỒỔỖỘƠỚỜỞỠỢÉÈẺẼẸÊẾỀỂỄỆÚÙỦŨỤƯỨỪỬỮỰÍÌỈĨỊÝỲỶỸỴĐ\")\n",
        "accepted_char=list((string.digits + ''.join(letters)))\n",
        "\n",
        "def call(sentence):\n",
        "    def encoder_data(text, maxlen=MAXLEN):\n",
        "            text = \"\\x00\" + text\n",
        "            x = np.zeros((maxlen, len(alphabet)))\n",
        "            for i, c in enumerate(text[:maxlen]):\n",
        "                x[i, alphabet.index(c)] = 1\n",
        "            if i < maxlen - 1:\n",
        "              for j in range(i+1, maxlen):\n",
        "                x[j, 0] = 1\n",
        "            return x\n",
        "\n",
        "    def decoder_data(x):\n",
        "        x = x.argmax(axis=-1)\n",
        "        return ''.join(alphabet[i] for i in x)\n",
        "\n",
        "    def nltk_ngrams(words, n=2):\n",
        "        return ngrams(words.split(), n)\n",
        "\n",
        "    def guess(ngram):\n",
        "        text = ' '.join(ngram)\n",
        "        preds = model.predict(np.array([encoder_data(text)]), verbose=0)\n",
        "        return decoder_data(preds[0]).strip('\\x00')\n",
        "\n",
        "    def correct(sentence):\n",
        "        for i in sentence:\n",
        "            if i not in accepted_char:\n",
        "                sentence=sentence.replace(i,\" \")\n",
        "        ngrams = list(nltk_ngrams(sentence, n=NGRAM))\n",
        "        guessed_ngrams = list(guess(ngram) for ngram in ngrams)\n",
        "\n",
        "        print(\"N gram\", ngrams)\n",
        "        print(\"guess\", guessed_ngrams)\n",
        "\n",
        "        candidates = [Counter() for _ in range(len(guessed_ngrams) + NGRAM - 1)]\n",
        "        for nid, ngram in (enumerate(guessed_ngrams)):\n",
        "            for wid, word in (enumerate(re.split(' +', ngram))):\n",
        "                candidates[nid + wid].update([word])\n",
        "\n",
        "        output = ' '.join(c.most_common(1)[0][0] for c in candidates)\n",
        "        return output\n",
        "\n",
        "    guess = correct(sentence)\n",
        "\n",
        "    return guess\n",
        "\n",
        "# sentence = \"Tôi là sinh vien trường đại họ cPhenikaa taji Hà Dông\"\n",
        "# sentence = 'Tôi phông iu thích hok ngôn ngữ mới'\n",
        "sentence = 'tôi teen laf Tô Văn Tus, tooi ddang hojc lớp 5'\n",
        "\n",
        "guess = call(sentence)\n",
        "print(guess)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NgyAcIJJVytR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}